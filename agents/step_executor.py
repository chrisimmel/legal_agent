from pydantic import BaseModel, Field

from .base import Agent
from llm import call_llm
from models import (
    AgentState,
    MAX_STEP_ATTEMPTS,
    StepExecutionAttempt,
    UserResponseDocument,
)
from storage.vector_store_manager import VectorStoreManager


class StepExecutorResearchLlmResponse(BaseModel):
    """
    A response from the research phase of the StepExecutorAgent.
    """

    query: str | None = Field(
        description="The query, if any, to be used to locate documents pertinent to the step."
    )


class StepExecutorExecutionLlmResponse(BaseModel):
    """
    A response from the execution phase of the StepExecutorAgent.
    """

    response_document: UserResponseDocument = Field(
        description="The response document generated by the step executor."
    )
    notes: str = Field(description="Notes about the step execution attempt.")


class StepExecutorAgent(Agent):
    """
    A step executor agent that executes a single step of the plan.
    """

    def __init__(self, user_query: str, vector_manager: VectorStoreManager):
        super().__init__("StepExecutorAgent", "Executes individual steps of the plan")
        self.user_query = user_query
        self.vector_manager = vector_manager

    async def execute_turn(
        self,
        agent_state: AgentState,
        attempt: int = 0,
        notes: str | None = None,
    ) -> AgentState:
        """
        Execute a single turn for the step executor agent.

        The purpose of this agent is to execute a single step of the plan.
        This requires three phases:
        1. An LLM call to generate a query to locate documents pertinent to the step.
        2. If a query was generated, make a call to the vector store to retrieve the documents.
        3. An LLM call to generate the state updates needed to complete the step. The context must include:
        - The original user query
        - The current state of the plan
        - The documents found in the vector store
        - Instructions to the LLM about how to update the plan state and the response document.
        """
        current_step = agent_state.plan.steps[agent_state.plan.current_step_index]

        print(
            f"ðŸ”„ Executing step {current_step.step_number} ({current_step.step_description}). Attempt {attempt}."
        )
        if attempt > 0:
            formatted_notes = f"""We are retrying this step after {attempt} of {MAX_STEP_ATTEMPTS} attempts.
Apply the following suggestions to correct remaining issues:
<SUGGESTIONS>
{notes}
</SUGGESTIONS>
"""
        else:
            formatted_notes = ""

        # Phase 1: Generate research query
        research_system_prompt = """
You are a research agent that creates search queries to find relevant documents when needed.

Given a user query, the full plan, and the current step, consider whether we need to look for pertinent documents.
If so, generate a focused search query that will help find the most relevant document chunks from a legal case database.
If no, leave the query empty.

The search query should be specific and use keywords that are likely to appear in relevant legal documents.
"""

        research_user_prompt = f"""
<USER_QUERY>
{self.user_query}
</USER_QUERY>

{formatted_notes}

<FULL_PLAN>
{agent_state.plan.model_dump_json(indent=2)}
</FULL_PLAN>

<CURRENT_STEP>
{current_step.model_dump_json(indent=2)}
</CURRENT_STEP>

<CURRENT_RESPONSE_DOCUMENT>
{agent_state.response_document.content}
</CURRENT_RESPONSE_DOCUMENT>

If appropriate, generate a search query to find documents relevant to this step.
"""

        research_response = await call_llm(
            research_system_prompt,
            research_user_prompt,
            StepExecutorResearchLlmResponse,
        )

        # Phase 2: Search vector store
        search_results = self.vector_manager.search(
            research_response.query, n_results=5
        )
        print(f"Found {len(search_results)} documents")

        # Format search results for LLM
        formatted_documents = "\n\n".join(
            [
                f"Document {i+1} (Case ID: {result.metadata['case_id']}, Similarity: {result.similarity:.3f}):\n{result.text}"
                for i, result in enumerate(search_results)
            ]
        )

        # Phase 3: Execute step with found documents
        execution_system_prompt = """
You are a step execution agent that processes legal documents to answer user queries.

Your job is to:
1. Analyze the provided document chunks
2. Extract relevant information for the current step
3. Update the response document with new findings
4. Provide detailed notes about what was accomplished

Be thorough and cite specific information from the documents. If no relevant information is found, state this clearly.
"""

        execution_user_prompt = f"""
<USER_QUERY>
{self.user_query}
</USER_QUERY>

<FULL_PLAN>
{agent_state.plan.model_dump_json(indent=2)}
</FULL_PLAN>

<CURRENT_STEP>
{current_step.model_dump_json(indent=2)}
</CURRENT_STEP>

<DOCUMENT_QUERY>
{research_response.query}
</DOCUMENT_QUERY>

<FOUND_DOCUMENTS>
{formatted_documents}
</FOUND_DOCUMENTS>

<CURRENT_RESPONSE_DOCUMENT>
{agent_state.response_document.content}
</CURRENT_RESPONSE_DOCUMENT>

Update the response document with relevant information from the found documents for this step. Provide notes about what was accomplished.
"""

        execution_response = await call_llm(
            execution_system_prompt,
            execution_user_prompt,
            StepExecutorExecutionLlmResponse,
        )

        # Update agent state
        agent_state.response_document = execution_response.response_document

        # Create execution attempt record
        attempt = StepExecutionAttempt(
            step_index=agent_state.plan.current_step_index,
            response_document=execution_response.response_document,
            executor_notes=execution_response.notes,
            success=False,  # Will be determined by reflector
            reflector_notes="",  # Will be filled by reflector
        )

        agent_state.history.append(attempt)
        return agent_state
